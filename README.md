# Optimizing Needlemanâ€“Wunsch Algorithm

This project explores various optimization techniques applied to the well-known Needlemanâ€“Wunsch algorithm.  
The Needlemanâ€“Wunsch algorithm is a dynamic programming method used to compute the optimal global alignment of two biological sequences (DNA, RNA, or proteins). It works by constructing a scoring matrix based on match, mismatch, and gap penalties, filling that matrix, and then tracing back to find the best alignment.

---

## Scoring Scheme

- **Match:**Â +1  
- **Mismatch:**Â âˆ’1  
- **Gap:**Â âˆ’2  

A match between characters is rewarded with +1, a mismatch costs âˆ’1, and introducing a gap (insertion/deletion) costs âˆ’2. This balances similarity against the cost of edits.

---

## 1. Matrix Initialization

We create an `(A+1) Ã— (B+1)` matrix (where `A` and `B` are the lengths of the two sequences), initialize the topâ€‘left cell to `0`, and fill the first row and column with cumulative gap penalties:

| Initial (0,0) = 0                                | First row & column with gap penalties        |
|--------------------------------------------------|----------------------------------------------|
| <img src="https://github.com/user-attachments/assets/0dbc59cb-2050-4334-9c8f-32d608ecdbb8" alt="Initial matrix" width="300"/> | <img src="https://github.com/user-attachments/assets/21dcb25a-0398-4a1b-9003-7d9907dd9b7f" alt="Row/column init" width="300"/> |

---

## 2. Matrix Filling

Starting from the **topâ€‘left**, for each cell `(i,j)` compute three candidate scores:

1. **Diagonal (â†–):**  `dp[i-1][j-1] + (matchâ€¯? +1Â : âˆ’1)`  
2. **Up (â†‘):**        `dp[i-1][j] âˆ’ 2`  
3. **Left (â†):**      `dp[i][j-1] âˆ’ 2`  

Take the **maximum** of those three and store it in `dp[i][j]`. Also record a **backâ€‘pointer** (â†–, â†‘, or â†) indicating which choice gave the max.

> **Visualization**  
> <img src="https://github.com/user-attachments/assets/9c61238d-efc5-4ae5-8da5-60eb6fc9e770" alt="Matrix filling" width="300"/>

---

## 3. Traceback

Once the matrix is full, start at the **bottomâ€‘right** cell and follow your backâ€‘pointers back to the **topâ€‘left**. Each pointer tells you whether to align a character from both sequences (â†–) or introduce a gap in one of them (â†‘ or â†). If two directions tie, there are multiple equallyâ€optimal alignments.

> **Traceback example**  
> <img src="https://github.com/user-attachments/assets/850a89f5-2cb0-40c4-90f1-62ccad2bcbf7" alt="Traceback" width="300"/>

---

Now we will look into methods of optimising the algorithm to find the minimum penalty or maximum score i.e the best alignment.
> **Note:** All benchmarks and performance measurements above were conducted on a sample dataset where both input sequences have length **10,000**.

## ProgramÂ 1: Columnâ€‘Traversal Benchmark

In **column.cpp**, we implement the Needlemanâ€“Wunsch DP matrix fill using **columnâ€‘traversal** over a rowâ€‘major layout. This stresses memory access patterns and lets us compare compiler/runtime performance:

- **Execution Time  (average of 10 runs)**  
  - g++: 750â€¯ms  

- **Throughput**  
  0.865â€¯GIPS (Giga Integer Operations Per Second)

You can view the full source code here:  
[ðŸ“„column.cpp](./optimizations/column.cpp)

## ProgramÂ 2: Rowâ€‘Major Traversal Benchmark

In **row.cpp**, we implement the Needlemanâ€“Wunsch DP matrix fill using **rowâ€‘traversal** over a rowâ€‘major layout. This change boosts spatial locality and cache hits.

- **Execution Time (average of 10 runs)**  
  - g++: 210â€¯ms

- **Throughput**  
  2.862â€¯GIPS  

You can view the full source and build instructions here:  
[ðŸ“„ row.cpp](./optimizations/row.cpp)

## ProgramÂ 3: Antiâ€‘Diagonal Traversal Benchmark

In **Anti-Diagonal.cpp**, we compute the scoring matrix by iterating along antiâ€‘diagonals (from the top edge toward the left edge). This approach isnâ€™t parallelized and has poor spatial locality, so it runs slower.

- **Execution Time (average of 10 runs)**  
  - g++: 730â€¯ms

- **Throughput**  
  0.822â€¯GIPS

<img src="https://github.com/user-attachments/assets/da61acdf-0ac8-47ce-b908-90be6fcff79a" alt="Initial matrix" width="200"/>


You can view the full source here:  
[ðŸ“„ Anti-Diagonal.cpp](./optimizations/Anti-Diagonal.cpp)

## ProgramÂ 4: Parallelized Antiâ€‘Diagonal + Loop Unrolling

In **unroll.cpp**, we parallelized the antiâ€‘diagonal traversal (since elements on the same antiâ€‘diagonal are independent) and added loop unrolling.

- **Execution Time (average of 10 runs)**  
  - g++: 142â€¯ms

- **Throughput**  
  4.225â€¯GIPS  

- **Using `#pragma unroll` for** innerâ€‘loop unrolling to increase instructionâ€‘level parallelism.

You can view the full source here:  
[ðŸ“„ unroll.cpp](./optimizations/unroll.cpp)

## ProgramÂ 5: Tiled Matrix Traversal

Tiling (or blocking) breaks the DP matrix into cacheâ€‘sized subâ€‘matrices (â€œtilesâ€) and processes one tile at a time, maximizing cache hits and minimizing mainâ€‘memory accesses.

In **tiling.cpp**, we explored tiling the DP matrix to improve spatial locality, boost cache hits, and reduce memory access time.

- **Execution Time (average of 10 runs)**  
  - g++: 350â€¯ms

- **Throughput**  
  1.714â€¯GIPS  

You can view the full source here:  
[ðŸ“„ tiling.cpp](./optimizations/tiling.cpp)

### ProgramÂ 6: Parallelized Antiâ€‘Diagonal Within Tiles

In **Anti-DiagonalTiling.cpp**, we apply the antiâ€‘diagonal computation **inside each cacheâ€‘sized tile** and parallelize across antiâ€‘diagonals as in ProgramÂ 4.

- **Execution Time (average of 10 runs)**  
  - g++: 363â€¯ms

- **Throughput**  
  1.652â€¯GIPS  

[ðŸ“„ Anti-DiagonalTiling.cpp](./optimizations/Anti-DiagonalTiling.cpp)

### ProgramÂ 7: Antiâ€‘Diagonal Across Tiles

In **AntiDiagonalTiles.cpp**, we apply antiâ€‘diagonal computation at the tile level: each tileâ€™s DP matrix is filled with the naive rowâ€‘major method, but tiles on the same antiâ€‘diagonal are processed in parallel. This yields our best performance yet.

- **Execution Time (average of 10 runs)**  
  - g++: 112â€¯ms (tile size 1000)  
  - g++: 64.5â€¯ms  (tile size 15â€“30)

- **Throughput**  
  - 5.358â€¯GIPS
  - 9.302â€¯GIPS 

[ðŸ“„ AntiDiagonalTiles.cpp](./optimizations/AntiDiagonalTiles.cpp)

### ProgramÂ 8: Nested Parallel Antiâ€‘Diagonal

In **AntiDiagonalTilesNested.cpp**, we apply the antiâ€‘diagonal approach both **across tiles** and **within each tile**, parallelizing every antiâ€‘diagonal of tiles and every antiâ€‘diagonal of cells inside those tiles. While this yields a solid improvement over the baseline, it doesnâ€™t outperform the previous tiled antiâ€‘diagonal strategy.

- **Execution Time (average of 10 runs)**  
  - g++: 162â€¯ms  

- **Throughput**  
  3.704â€¯GIPS  

[ðŸ“„ AntiDiagonalTilesNested.cpp](./optimizations/AntiDiagonalTilesNested.cpp)


---

> **For final observations and conclusions, see [OBSERVATIONS.md](./OBSERVATIONS.md).**









